# Azure ML Scoring Script Documentation

This directory contains comprehensive documentation for the Azure ML scoring script (`main.py`) used in managed online endpoint deployments.

## ğŸ“ Files

| File | Purpose |
|------|---------|
| `main.py` | The actual scoring script that handles predictions |
| `main.mermaid` | Complete lifecycle diagram (deployment â†’ requests â†’ shutdown) |
| `main-timeline.mermaid` | Timeline view showing when each phase occurs |
| `main-functions.mermaid` | Detailed function call sequence diagram |

## ğŸ¯ Quick Summary

### What is `main.py`?
A Python script that acts as a "bridge" between HTTP requests and your ML model. It has two main functions:

#### 1. `init()` - Called ONCE during deployment
```python
def init():
    """Load model into memory when container starts"""
    global model
    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'model')
    model = mlflow.sklearn.load_model(model_path)
```

**When**: Container startup (deployment phase)  
**Time**: ~5 seconds  
**Purpose**: Load model into RAM for fast predictions  
**Frequency**: Once per deployment

#### 2. `run(raw_data)` - Called for EVERY prediction request
```python
def run(raw_data):
    """Make predictions using the loaded model"""
    data = json.loads(raw_data)
    predictions = model.predict(data['input_data']['data'])
    return json.dumps({"predictions": predictions.tolist()})
```

**When**: Every HTTP POST to /score  
**Time**: ~100ms per request  
**Purpose**: Process data and return predictions  
**Frequency**: Multiple times (hundreds/thousands of requests)

## ğŸ“Š Visual Documentation

### 1. Complete Lifecycle Diagram (`main.mermaid`)
Shows the entire process from deployment to shutdown, including:
- Container creation and setup
- Model loading (init)
- Request handling (run)
- Container running state
- Shutdown process

**Key Sections**:
- Phase 1: Deployment (one-time, 2-5 minutes)
- Phase 2: Container Running (continuous)
- Phase 3: Prediction Requests (multiple, ~100ms each)
- Phase 4: Shutdown (when deleted)

### 2. Timeline Diagram (`main-timeline.mermaid`)
Gantt chart showing:
- When each phase occurs
- How long each operation takes
- Parallel operations (container running + handling requests)

**Timing Breakdown**:
- Deployment: ~75-80 seconds (one time)
- Each prediction: ~75-100ms (many times)

### 3. Function Call Flow (`main-functions.mermaid`)
Sequence diagram showing:
- Detailed interaction between components
- Function call order
- Data flow through the system
- How model stays in memory

## ğŸš€ How It Works

### Deployment Flow
```
Developer â†’ Azure ML â†’ Create Container â†’ Load Model (init) â†’ Ready!
            2-5 minutes                        5 seconds
```

### Prediction Flow
```
Client â†’ HTTP POST â†’ Web Server â†’ run(data) â†’ Model Prediction â†’ Response
                     ~10ms         ~50ms        ~50ms            ~10ms
                                   Total: ~100ms
```

### Key Insight
```
âŒ BAD:  Load model â†’ Predict â†’ Unload (5 seconds per request)
âœ… GOOD: Load once â†’ Predict â†’ Predict â†’ Predict (100ms per request)
```

## ğŸ“ˆ Performance Impact

| Approach | Time per Request | Reason |
|----------|------------------|--------|
| **Reload model each time** | ~5-10 seconds | Loading .pkl file from disk is slow |
| **Load once in init()** | ~100ms | Model already in RAM, instant access |

**Benefit**: 50-100x faster predictions!

## ğŸ”„ Container Lifecycle

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DEPLOYMENT (Once)                       â”‚
â”‚ - Create container: 30s                 â”‚
â”‚ - Download model: 30s                   â”‚
â”‚ - Start web server: 5s                  â”‚
â”‚ - Call init(): 5s â†’ Model in RAM       â”‚
â”‚ Total: ~75-80 seconds                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RUNNING STATE (Continuous)              â”‚
â”‚ - Container: Running                    â”‚
â”‚ - Web server: Listening on port 31311  â”‚
â”‚ - Model: Loaded in RAM                  â”‚
â”‚ - Status: Ready for requests            â”‚
â”‚ Duration: Hours/Days/Weeks              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ HANDLING REQUESTS (Multiple)            â”‚
â”‚ Request 1: run() â†’ 100ms â†’ Response    â”‚
â”‚ Request 2: run() â†’ 100ms â†’ Response    â”‚
â”‚ Request 3: run() â†’ 100ms â†’ Response    â”‚
â”‚ ... thousands of requests ...           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SHUTDOWN (When deleted)                 â”‚
â”‚ - Stop web server                       â”‚
â”‚ - Terminate container                   â”‚
â”‚ - Clear memory                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ’¡ Common Misconceptions

### âŒ WRONG: "The script runs for every prediction"
âœ… CORRECT: The container runs continuously, only `run()` is called per request

### âŒ WRONG: "The model is loaded for each request"
âœ… CORRECT: Model is loaded ONCE in `init()`, then reused for all requests

### âŒ WRONG: "The web server restarts for each request"
âœ… CORRECT: Web server stays running, just processes new requests

## ğŸ” Debugging Tips

### Check Deployment Logs
```bash
az ml online-deployment get-logs \
  --name your-deployment \
  --endpoint-name your-endpoint \
  --lines 100
```

**Look for**:
- âœ… "Model loaded successfully!" â†’ init() worked
- âœ… "Predictions: [0 1 0]" â†’ run() is working
- âŒ "Failed to initialize model" â†’ init() failed
- âŒ "Error making predictions" â†’ run() failed

### Common Issues
1. **Container crashes immediately**: init() failed to load model
2. **Slow first request**: Normal (model loading in init())
3. **Slow subsequent requests**: Problem with run() or model size
4. **404 errors**: Endpoint/deployment not ready yet

## ğŸ“ Input/Output Format

### Request Format
```json
{
  "input_data": {
    "columns": ["Pregnancies", "PlasmaGlucose", "DiastolicBloodPressure", 
                "TricepsThickness", "SerumInsulin", "BMI", "DiabetesPedigree", "Age"],
    "data": [
      [9, 104, 51, 7, 24, 27.36983156, 1.350472047, 43],
      [6, 73, 61, 35, 24, 18.74367404, 1.074147566, 75]
    ]
  }
}
```

### Response Format
```json
{
  "predictions": [0, 1]
}
```
Where: `0` = No diabetes, `1` = Has diabetes

## ğŸ”— Related Files

- `deployment-blue.yml` - References main.py in code_configuration
- `deployment-green.yml` - References main.py for green deployment
- `train.py` - Creates the model that main.py loads
- `../deployment/test_endpoint.py` - Tests the deployed endpoint

## ğŸ“š Further Reading

- [Azure ML Managed Online Endpoints](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-managed-online-endpoints)
- [MLflow Models](https://mlflow.org/docs/latest/models.html)
- [Scoring Script Best Practices](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-advanced-entry-script)

---

**Created**: 2025-10-09  
**Purpose**: Documentation for Azure ML scoring script lifecycle  
**Diagrams**: View .mermaid files in a Mermaid viewer or GitHub

